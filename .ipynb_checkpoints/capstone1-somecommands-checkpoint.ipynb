{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "df_copy = df.copy()\n",
    "\n",
    "# Sort the RUCC column\n",
    "df_copy.sort_values(['RUCC'],inplace=True)\n",
    "\n",
    "#Melt the dataframe based on the four education levels\n",
    "df_melt = pd.melt(df_copy,id_vars='RUCC',value_vars=['less_than_high_school','high_school_diploma','college/associate_degree',\n",
    "                                                     'bachelors/higher'], var_name='education_level',value_name='percentage')\n",
    "\n",
    "# Create the boxplot of the four education level for each RUCC\n",
    "figure, ax = plt.subplots(figsize=(20,10))\n",
    "sns.boxplot(ax=ax, x='education_level',y='percentage',hue='RUCC',data=df_melt,fliersize=2,width=0.6)\n",
    "ax.legend(loc='lower left',bbox_to_anchor=(0.47,0.55),fontsize=16,title='RUCC')\n",
    "ax.set_ylim(0,80)\n",
    "ax.set_title('Box plot of the four education levels of counties across the nine rural urban continuum codes (RUCC) in US',\n",
    "             fontsize = 20, y=1.02)\n",
    "ax.set_xlabel('education level',fontsize = 20)\n",
    "ax.set_ylabel('percentage',fontsize = 20)\n",
    "ax.tick_params(axis='both', labelsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Story Telling section, it was shown that the correlations between poverty and majority of rural urban continuum codes \n",
    "# i.e. all except RUCC 1 and 6, are close to each other . Therefore, any of these seven RUCC (2-5,7-9) could be picked as the \n",
    "# base to simplify the interpretation. I pick RUCC 9 and remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the linear model with OLS command\n",
    "X2 = sm.add_constant(X) #OLS by default does not have intercept\n",
    "model = sm.OLS(y,X2).fit()\n",
    "\n",
    "# Remove predcitors with p-value less than 0.001\n",
    "X_new = X2.loc[:,model.pvalues[model.pvalues <0.001].index]\n",
    "\n",
    "# fit the model on the dataset with reduced number of predictors\n",
    "model = sm.OLS(y,X_new).fit()\n",
    "\n",
    "# print summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Remove predcitors with p-value less than 0.001\n",
    "X_new = X2.loc[:,model.pvalues[model.pvalues <0.001].index]\n",
    "\n",
    "# fit the model on the dataset with reduced number of predictors\n",
    "model = sm.OLS(y,X_new).fit()\n",
    "\n",
    "# print summary of the model\n",
    "model.summary()\n",
    "\n",
    "# Plot residuals vs. fitted values\n",
    "figure,ax = plt.subplots(figsize=(10,6))\n",
    "ax.scatter(model.fittedvalues,model.resid)\n",
    "ax.set_xlabel('Fitted values')\n",
    "ax.set_ylabel('Residuals');\n",
    "ax.set_title('Residuals vs. fitted values',y=1.02)\n",
    "ax.axis([0,45,-35,35])\n",
    "\n",
    "# Plot quantile plot\n",
    "figure,ax = plt.subplots(figsize=(10,6))\n",
    "stats.probplot(model.resid,plot=ax)\n",
    "ax.set_ylabel('Residual quantiles')\n",
    "ax.set_title('Quantile-Quantile plot',y=1.02)\n",
    "ax.axis([-4,4,-35,35])\n",
    "\n",
    "# Show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the influence array and create a panda peries with the external studentized residuals\n",
    "infl = model.get_influence()\n",
    "p = pd.Series(infl.resid_studentized_external)\n",
    "\n",
    "# Extract the indices of the series for which the absolute value of external studentized residual is more than 3.\n",
    "indices = p[np.abs(p)>3].index\n",
    "\n",
    "# Create a dataframe from the original dataframe which only include the indices extracted above\n",
    "df_outliers = df.iloc[indices]\n",
    "X_reduced=X_new.iloc[p[np.abs(p)<=3].index]\n",
    "y_reduced=y.iloc[p[np.abs(p)<=3].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DummyRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "# Build the dummy classifier\n",
    "clf = DummyRegressor(strategy = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataframe for dummy variables representing the nine RUCC and concat it to the original dataframe\n",
    "ohe = OneHotEncoder()\n",
    "RUCC_matrix = ohe.fit_transform(df.RUCC.values.reshape(-1,1)).toarray()\n",
    "RUCC_df = pd.DataFrame(RUCC_matrix,columns = ['RUCC_'+str(i) for i in np.arange(1,10)])\n",
    "df_concat = pd.concat([df_copy,RUCC_df],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Order the data frame based on South, Northeast, Midwest, and West.\n",
    "df.region = pd.Categorical(df.region, categories=[\"South\",\"Northeast\",\"Midwest\",\"West\"],ordered=True)\n",
    "df.sort_values('region',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df_concat[df_concat['2013 Rural-urban Continuum Code'].isnull()].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>year</th>\n",
       "      <th>1949</th>\n",
       "      <th>1950</th>\n",
       "      <th>1951</th>\n",
       "      <th>1952</th>\n",
       "      <th>1953</th>\n",
       "      <th>1954</th>\n",
       "      <th>1955</th>\n",
       "      <th>1956</th>\n",
       "      <th>1957</th>\n",
       "      <th>1958</th>\n",
       "      <th>1959</th>\n",
       "      <th>1960</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>January</th>\n",
       "      <td>112</td>\n",
       "      <td>115</td>\n",
       "      <td>145</td>\n",
       "      <td>171</td>\n",
       "      <td>196</td>\n",
       "      <td>204</td>\n",
       "      <td>242</td>\n",
       "      <td>284</td>\n",
       "      <td>315</td>\n",
       "      <td>340</td>\n",
       "      <td>360</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>February</th>\n",
       "      <td>118</td>\n",
       "      <td>126</td>\n",
       "      <td>150</td>\n",
       "      <td>180</td>\n",
       "      <td>196</td>\n",
       "      <td>188</td>\n",
       "      <td>233</td>\n",
       "      <td>277</td>\n",
       "      <td>301</td>\n",
       "      <td>318</td>\n",
       "      <td>342</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>March</th>\n",
       "      <td>132</td>\n",
       "      <td>141</td>\n",
       "      <td>178</td>\n",
       "      <td>193</td>\n",
       "      <td>236</td>\n",
       "      <td>235</td>\n",
       "      <td>267</td>\n",
       "      <td>317</td>\n",
       "      <td>356</td>\n",
       "      <td>362</td>\n",
       "      <td>406</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>April</th>\n",
       "      <td>129</td>\n",
       "      <td>135</td>\n",
       "      <td>163</td>\n",
       "      <td>181</td>\n",
       "      <td>235</td>\n",
       "      <td>227</td>\n",
       "      <td>269</td>\n",
       "      <td>313</td>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "      <td>396</td>\n",
       "      <td>461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>May</th>\n",
       "      <td>121</td>\n",
       "      <td>125</td>\n",
       "      <td>172</td>\n",
       "      <td>183</td>\n",
       "      <td>229</td>\n",
       "      <td>234</td>\n",
       "      <td>270</td>\n",
       "      <td>318</td>\n",
       "      <td>355</td>\n",
       "      <td>363</td>\n",
       "      <td>420</td>\n",
       "      <td>472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "year      1949  1950  1951  1952  1953  1954  1955  1956  1957  1958  1959  \\\n",
       "month                                                                        \n",
       "January    112   115   145   171   196   204   242   284   315   340   360   \n",
       "February   118   126   150   180   196   188   233   277   301   318   342   \n",
       "March      132   141   178   193   236   235   267   317   356   362   406   \n",
       "April      129   135   163   181   235   227   269   313   348   348   396   \n",
       "May        121   125   172   183   229   234   270   318   355   363   420   \n",
       "\n",
       "year      1960  \n",
       "month           \n",
       "January    417  \n",
       "February   391  \n",
       "March      419  \n",
       "April      461  \n",
       "May        472  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights = flights.pivot(\"month\", \"year\", \"passengers\")\n",
    "flights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cross tabulation: median of more_than_high_school in each region-RUCC pair')\n",
    "df_copy.pivot_table(index='region',columns='RUCC',values='more_than_high_school',aggfunc='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group bu RUCC and measure the average\n",
    "df_group = df.groupby('RUCC').mean().loc[:,['high_school_or_less','poverty','unemployment']]\n",
    "\n",
    "# Plot the average of less than high school, poverty, and unemployment across the nine RUCC\n",
    "plt.plot(df_group,'--D')\n",
    "plt.legend(df_group.columns,loc='lower left',bbox_to_anchor=(1,0))\n",
    "plt.xlabel('RUCC',fontsize = 20)\n",
    "plt.ylabel('percentage',fontsize = 20)\n",
    "plt.title('Average percentage of population with high school diploma or less,\\n \\\n",
    "poverty and unemployment across nine RUCC',fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define clustering function\n",
    "def clustering(X_train,X_test,n_cluster):\n",
    "    \n",
    "    # Build the KMeans cluster, fit it to the train set, create the new feature for the train set, \n",
    "    # cocatenate it to the features DataFrame, transform it to dummy variables, and remove the last dummy variable \n",
    "    kmeans = KMeans(n_clusters=n_cluster,random_state=21)\n",
    "    kmeans.fit(X_train)\n",
    "    new_column_train = kmeans.predict(X_train) \n",
    "    new_df_train = pd.DataFrame(new_column_train,columns = ['cluster'], index = X_train.index)\n",
    "    X_train_extended = pd.concat([X_train,new_df_train],axis=1)\n",
    "    X_train_extended.cluster = X_train_extended.cluster.astype('str')\n",
    "    X_train_extended = pd.get_dummies(X_train_extended)\n",
    "    X_train_extended.drop(['cluster_{}'.format(n_cluster-1)],axis=1,inplace=True)\n",
    "    \n",
    "    # Build the new feature for the test set, concatenate it to the features DataFrame of the test set\n",
    "    # transform it to dummy variables, and remove the last dummy variable \n",
    "    new_column_test = kmeans.predict(X_test) \n",
    "    new_df_test = pd.DataFrame(new_column_test,columns = ['cluster'], index = X_test.index)\n",
    "    X_test_extended = pd.concat([X_test,new_df_test],axis=1)\n",
    "    X_test_extended.cluster = X_test_extended.cluster.astype('str')\n",
    "    X_test_extended = pd.get_dummies(X_test_extended)\n",
    "    X_test_extended.drop(['cluster_{}'.format(n_cluster-1)],axis=1,inplace=True)\n",
    "    \n",
    "    # Return the new features DataFrame for both train and test sets\n",
    "    return X_train_extended,X_test_extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_forest_SearchCV(cluster_numbers,n_estimators,min_samples_leaf,criterion,max_features,X_train,y_train):\n",
    "    \n",
    "    # Create KFold with 5 as the number of splits.\n",
    "    kf = KFold(n_splits = 5)\n",
    "    # Create a zero array which will hold the score of each split run. Since the number of splits is 5, the first dimention is 5\n",
    "    a = np.zeros(shape = (5,len(cluster_numbers),len(n_estimators),len(min_samples_leaf),len(criterion),len(max_features)))\n",
    "    \n",
    "    t = 0\n",
    "    # Loop over the splits of the train set  \n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_train_kf = X_train.iloc[train_index]\n",
    "        X_test_kf = X_train.iloc[test_index]\n",
    "        y_train_kf = y_train.iloc[train_index]\n",
    "        y_test_kf = y_train.iloc[test_index]\n",
    "        \n",
    "        # Loop over number of clusters\n",
    "        for i,n_cluster in enumerate(cluster_numbers):\n",
    "            # Cluster each dataset in train set \n",
    "            X_train_kf_extended,X_test_kf_extended = clustering(X_train_kf,X_test_kf,n_cluster)\n",
    "            # Loop over n_estimators\n",
    "            for j,n_estimator in enumerate(n_estimators):\n",
    "                # Loop over min_samples_leaf\n",
    "                for k,min_sample_leaf in enumerate(min_samples_leaf):\n",
    "                    # Loop over criterion\n",
    "                    for l,criteria in enumerate(criterion):\n",
    "                        # Loop over max features\n",
    "                        for m,max_feature in enumerate (max_features):\n",
    "                            # Create the random forest and find the score\n",
    "                            model = RandomForestRegressor(n_estimators = n_estimator, min_samples_leaf = min_sample_leaf,\n",
    "                                                   criterion = criteria, max_features = max_feature, random_state=21)\n",
    "                            model.fit(X_train_kf_extended,y_train_kf)\n",
    "                            score = model.score(X_test_kf_extended,y_test_kf)\n",
    "                            # Save the score in the array\n",
    "                            a[t,i,j,k,l,m] = score\n",
    "        t += 1\n",
    "     \n",
    "    # Average the scores \n",
    "    average_score = np.mean(a,axis = 0)\n",
    "    \n",
    "    \n",
    "    #Find the highest average score\n",
    "    max_score = np.amax(average_score)\n",
    "    \n",
    "    # Find the cluster_number, n_estimator, and min_sample_leaf corresponding to the best score\n",
    "    max_score_index = np.unravel_index(average_score.argmax(), average_score.shape)\n",
    "    cluster_number = cluster_numbers[max_score_index[0]]\n",
    "    n_estimator = n_estimators[max_score_index[1]]\n",
    "    min_sample_leaf = min_samples_leaf[max_score_index[2]]\n",
    "    criteria = criterion[max_score_index[3]]\n",
    "    max_feature = max_features[max_score_index[4]]\n",
    "    \n",
    "    # Print the best score of parameter tuning and best parameters\n",
    "    print('The best score is %.3f'%max_score)\n",
    "    print('\\nThe best parameters are:')\n",
    "    print('Number of clusters: ',cluster_number)\n",
    "    print('n_estimators: ',n_estimator)\n",
    "    print('min_samples_leaf: ',min_sample_leaf)\n",
    "    print('criterion: ',criteria)\n",
    "    print('max_features: ',max_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
